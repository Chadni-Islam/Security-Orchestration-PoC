{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://spacy.io/usage/linguistic-features\n",
    "# https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/\n",
    "# https://nlpforhackers.io/complete-guide-to-spacy/\n",
    "import spacy\n",
    "import pandas as pd\n",
    "#for visualization of Entity detection importing displacy from spacy\n",
    "from spacy import displacy\n",
    "import nltk\n",
    "# https://www.analyticsvidhya.com/blog/2019/02/stanfordnlp-nlp-library-python\n",
    "# sentence tokenisation Load English tokenizer, tagger, parser, NER and word vectors\n",
    "# nlp = English()\n",
    "\n",
    "import en_core_web_sm\n",
    "# import en_core_web_lg\n",
    "from spacy.pipeline import Sentencizer\n",
    "sentencizer = Sentencizer()\n",
    "# Create the pipeline 'sentencizer' component\n",
    "# sbd = nlp.create_pipe('sentencizer')\n",
    "# Add the component to the pipeline\n",
    "# nlp.add_pipe(sbd)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(sentencizer, first=True)\n",
    "\n",
    "# dependency\n",
    "root = 'root'\n",
    "direct_obj = 'dobj' \n",
    "nominal_subj = 'nsubj' \n",
    "prep = 'prep'\n",
    "clausal_complement = ['ccomp', 'advcl']\n",
    "compound = 'compound'\n",
    "list_dep = ['compound', 'pobj', root, nominal_subj, direct_obj]\n",
    "# preposition\n",
    "verb = 'VERB'\n",
    "noun = 'NOUN'\n",
    "root = 'ROOT'\n",
    "adp = 'ADP'\n",
    "verb_noun = [verb, noun, 'PRON']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_playbook(filename, column):\n",
    "    # read playbook or task name\n",
    "    df_content = pd.read_csv(filename)\n",
    "    column_content = df_content[column]\n",
    "      # drop empty cell\n",
    "    column_content = column_content.dropna()\n",
    "    # drop duplicates cells\n",
    "    column_content = column_content.drop_duplicates() \n",
    "    # print(playbook_name.head())\n",
    "    # print('\\n *********************************** \\n')\n",
    "    return column_content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_sentences(sentences, nlp):\n",
    "    # first separate the sentence in the playbok\n",
    "    sentence_list = []\n",
    "    for sentences in sentences.values:\n",
    "        # \"nlp\" Object is used to create documents with linguistic annotations.\n",
    "        doc = nlp(sentences)\n",
    "        # create list of sentence tokens\n",
    "        for sent in doc.sents: \n",
    "            sentence_list.append(sent.text.lower())\n",
    "            # print(sent.text, '\\n')\n",
    "    print('Number of sentence: ', len(sentence_list))\n",
    "    return sentence_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentence:  1041\n"
     ]
    }
   ],
   "source": [
    "# read playbook details\n",
    "playbook_name = read_playbook('Playbook.csv', 'name')\n",
    "# read task details\n",
    "task_plan = read_playbook('Task_playbook.csv', 'description')\n",
    "# read task name\n",
    "task_name = read_playbook('Task_playbook.csv', 'name')\n",
    "sentence_list = []\n",
    "# first separate the sentence in the playbok\n",
    "sentence_list = separate_sentences(task_plan, nlp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_api_param(token, token_comp):\n",
    "    # token_comp = token.lemma_\n",
    "    for child in token.children:\n",
    "        if child.dep_== 'compound':\n",
    "            new_str = str(child.lemma_ + '.' + token.lemma_)\n",
    "            old_str = str(token.lemma_)\n",
    "            token_comp = token_comp.replace(old_str, new_str)\n",
    "            # print('compound:', token_comp)\n",
    "            for token1 in child.children:\n",
    "                if token1.dep_ == 'compound':\n",
    "                    token_comp = calculate_api_param(child, token_comp)\n",
    "    return token_comp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_compound(token, token_comp):\n",
    "    # token_comp = token.lemma_\n",
    "    for child in token.children:\n",
    "        if child.dep_== 'compound' or child.dep_=='amod':\n",
    "            # token_comp = str(child.lemma_ + '.' + token_comp)\n",
    "            new_str = str(child.lemma_ + '.' + token.lemma_)\n",
    "            token_comp = token_comp.replace(str(token.lemma_), new_str)\n",
    "            # print('compound:', token_comp)\n",
    "            for token1 in child.children:\n",
    "                if token1.dep_ == 'compound':\n",
    "                    token_comp = calculate_compound(child, token_comp)\n",
    "    return token_comp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify First API - the task that need to be done\n",
    "def generate_api(doc):   \n",
    "    first_api = second_api = third_api = first_api_compound = second_api_compound = third_api_compound = \"\"\n",
    "    for token in doc:\n",
    "        f = s = t =v = 0.00\n",
    "        token_dep = token.dep_\n",
    "        token_pos = token.pos_    \n",
    "        child = [i for i in token.children]\n",
    "        # print('token_dep: ', token_dep, 'token_pos: ', token_pos)\n",
    "        # rules for identifying first API\n",
    "        if token_dep == root and token_pos in verb_noun: # rule 1    \n",
    "            first_api = token.lemma_\n",
    "            print('rule 1: ', first_api)\n",
    "            f = 1\n",
    "            for child in token.children:\n",
    "                if child.dep_ == compound and child.pos_ in verb_noun: # identify the compound of root\n",
    "                    first_api = child.lemma_\n",
    "                    first_api_compound = calculate_compound(child, child.lemma_)\n",
    "                    first_api_compound = first_api_compound.replace(child.lemma_, \"\")\n",
    "                    second_api = token.lemma_\n",
    "                    second_api_compound = calculate_compound(token, token.lemma_)\n",
    "                    second_api_compound = second_api_compound.replace(token.lemma_, \"\")\n",
    "                    print('rule 2.1: ', first_api, '.', second_api)\n",
    "                    print('first_api_param', first_api_compound,' second_api_param ', second_api_compound)\n",
    "                    f = s = 2.1\n",
    "                elif child.dep_ in [nominal_subj, 'amod']  and child.pos_ in [verb_noun, 'ADJ']: # identify the subject of root\n",
    "                    children_child = [i for i in child.children]\n",
    "                    if(len(children_child) == 0):\n",
    "                        first_api =  child.lemma_\n",
    "                        first_api_compound = calculate_compound(child, child.lemma_)\n",
    "                        first_api_compound = first_api_compound.replace(child.lemma_, \"\")\n",
    "                        second_api = token.lemma_\n",
    "                        second_api_compound = calculate_compound(token, token.lemma_)\n",
    "                        second_api_compound = second_api_compound.replace(token.lemma_, \"\")\n",
    "                        print('rule 2.2: ', first_api, '.', second_api)\n",
    "                        print('first_api_param', first_api_compound,' second_api_param ', second_api_compound)\n",
    "                        f = s = 2.2\n",
    "                elif child.dep_ == 'dep'  and child.pos_ in verb_noun: # identify the subject of root\n",
    "                    children_child = [i for i in child.children]\n",
    "                    if(len(children_child) == 0):\n",
    "                        third_api = second_api\n",
    "                        third_api_compound = second_api_compound\n",
    "                        second_api = first_api\n",
    "                        second_api_compound = first_api_compound\n",
    "                        first_api = child.lemma_ \n",
    "                        first_api_compound = calculate_compound(child, child.lemma_)\n",
    "                        first_api_compound = first_api_compound.replace(child.lemma_, \"\")\n",
    "                        print('rule 2.3: ', first_api, '.', second_api)\n",
    "                        print('first_api_param', first_api_compound,' second_api_param ', second_api_compound,'third_api_param ', third_api_compound)\n",
    "                        f = s = 2.3\n",
    "                elif child.dep_ in clausal_complement and child.pos_ == verb:\n",
    "                    for token1 in child.children:\n",
    "                        if token1.dep_ == nominal_subj:\n",
    "                            second_api = token1.lemma_\n",
    "                            second_api_compound = calculate_compound(token1, token1.lemma_)\n",
    "                            second_api_compound = second_api_compound.replace(token1.lemma_, \"\")\n",
    "                            print('rule 2.5: ', first_api, '. ', second_api)\n",
    "                            print('second_api_param', second_api_compound)\n",
    "                            f = s = 2.5\n",
    "                        elif token1.dep_== direct_obj:\n",
    "                            third_api = token1.lemma_  \n",
    "                            third_api_compound = calculate_compound(token1, token1.lemma_)\n",
    "                            third_api_compound = third_api_compound.replace(token1.lemma_, \"\")\n",
    "                            print('rule 3.5: ', first_api, '. ', second_api, '. ', third_api)\n",
    "                            print('third_api_param', third_api_compound)\n",
    "                            t = 3.5\n",
    "                elif child.dep_ == direct_obj and child.pos_ in verb_noun:\n",
    "                    if s == 0:\n",
    "                        second_api = child.lemma_\n",
    "                        second_api_compound = calculate_compound(child, child.lemma_)\n",
    "                        second_api_compound = second_api_compound.replace(child.lemma_, \"\")\n",
    "                        print('rule 2.4: ', first_api, '. ', second_api) # rule 2\n",
    "                        print('second_api_param', second_api_compound)\n",
    "                        s = 2.4\n",
    "                        for token1 in child.children:\n",
    "                            if token1.dep_ == 'prep' or 'adative' and token1.pos_ == 'ADP':\n",
    "                                for token2 in token1.children:\n",
    "                                    third_api = token2.lemma_\n",
    "                                    t = 3.41\n",
    "                                    print('rule 3.1: ', first_api, '. ', second_api, '.', third_api)\n",
    "                                    third_api_compound = calculate_compound(token2, token2.lemma_)\n",
    "                                    third_api_compound = third_api_compound.replace(token2.lemma_, \"\")\n",
    "                                    print('third_api_param', third_api_compound)\n",
    "                    else:\n",
    "                        third_api = child.lemma_\n",
    "                        third_api_compound = calculate_compound(child, child.lemma_)\n",
    "                        third_api_compound = third_api_compound.replace(child.lemma_, \"\")\n",
    "                        print('rule 3.4: ', first_api, '. ', second_api, '.', third_api) # rule 2\n",
    "                        print('third_api_param', third_api_compound)\n",
    "                        t = 3.4\n",
    "                elif child.dep_ == 'prep' or 'adative' and child.pos_ == 'ADP':\n",
    "                    for token1 in child.children:\n",
    "                        if token1.dep_ == 'pobj' and token1.pos_ in verb_noun:\n",
    "                            if s == 0:\n",
    "                                second_api = token1.lemma_\n",
    "                                second_api_compound = calculate_compound(token1, token1.lemma_)\n",
    "                                print('rule 2.1: ', first_api, '. ', second_api)\n",
    "                                s = 2.1\n",
    "                                print('second_api_param', second_api_compound)\n",
    "                            else:\n",
    "                                third_api = token1.lemma_\n",
    "                                t = 3.1\n",
    "                                print('rule 3.1: ', first_api, '. ', second_api, '.', third_api)\n",
    "                                third_api_compound = calculate_compound(token1, token1.lemma_)\n",
    "                                third_api_compound = third_api_compound.replace(token1.lemma_, \"\")\n",
    "                                print('third_api_param', third_api_compound)\n",
    "                elif child.dep_ in ['pobj', 'advcl'] and child.pos_ in ['ADJ', 'NOUN', 'PRON']:\n",
    "                    if t == 0:\n",
    "                        third_api = child.lemma_\n",
    "                        #print('pobject', third_param)\n",
    "                        t = 3.6\n",
    "                        print('rule 3.6: ', first_api, '. ', second_api, '.', third_api)\n",
    "                        third_api_compound = calculate_compound(child, child.lemma_)\n",
    "                        third_api_compound = third_api_compound.replace(child.lemma_, \"\")\n",
    "                        print('third_api_param', third_api_compound)\n",
    "                                \n",
    "    return first_api, second_api, third_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " check if there is any duplicate incident found. \n",
      "\n",
      "rule 1:  check\n",
      "check . \n",
      "\n",
      " find duplicate incidents using machine-learning. \n",
      "\n",
      "rule 1:  find\n",
      "rule 2.4:  find .  incident\n",
      "second_api_param duplicate.\n",
      "find . incident\n",
      "\n",
      " check if the candidate for duplicate incident score is above the threshold. \n",
      "\n",
      "rule 1:  check\n",
      "rule 2.5:  check .  candidate\n",
      "second_api_param \n",
      "check . candidate\n",
      "\n",
      " checks if incident has 'email/from' label \n",
      "\n",
      "rule 1:  check\n",
      "rule 2.5:  check .  incident\n",
      "second_api_param \n",
      "rule 3.5:  check .  incident .  email\n",
      "third_api_param \n",
      "check . incident . email\n",
      "\n",
      " parse email with splunk \n",
      "\n",
      "rule 1:  email\n",
      "rule 2.2:  parse . email\n",
      "first_api_param   second_api_param  parse.\n",
      "rule 3.1:  parse .  email . splunk\n",
      "third_api_param \n",
      "parse . email . splunk\n",
      "\n",
      " parse email with nexpose \n",
      "\n",
      "rule 1:  email\n",
      "rule 2.2:  parse . email\n",
      "first_api_param   second_api_param  parse.\n",
      "rule 3.1:  parse .  email . nexpose\n",
      "third_api_param \n",
      "parse . email . nexpose\n",
      "\n",
      " parse email with sentinelone \n",
      "\n",
      "rule 1:  email\n",
      "rule 2.2:  parse . email\n",
      "first_api_param   second_api_param  parse.\n",
      "rule 3.1:  parse .  email . sentinelone\n",
      "third_api_param \n",
      "parse . email . sentinelone\n",
      "\n",
      " classify incident with qradar \n",
      "\n",
      "rule 1:  incident\n",
      "rule 2.2:  classify . incident\n",
      "first_api_param   second_api_param  classify.\n",
      "rule 3.1:  classify .  incident . qradar\n",
      "third_api_param \n",
      "classify . incident . qradar\n",
      "\n",
      " prints text to war room \n",
      "\n",
      "rule 1:  print\n",
      "rule 2.4:  print .  text\n",
      "second_api_param \n",
      "rule 3.1:  print .  text . room\n",
      "third_api_param war.\n",
      "print . text . room\n",
      "\n",
      " classify incident with vectra \n",
      "\n",
      "rule 1:  incident\n",
      "rule 2.2:  classify . incident\n",
      "first_api_param   second_api_param  classify.\n",
      "rule 3.1:  classify .  incident . vectra\n",
      "third_api_param \n",
      "classify . incident . vectra\n",
      "\n",
      " classify incident with trendmicro \n",
      "\n",
      "rule 1:  incident\n",
      "rule 2.2:  classify . incident\n",
      "first_api_param   second_api_param  classify.\n",
      "rule 3.1:  classify .  incident . trendmicro\n",
      "third_api_param \n",
      "classify . incident . trendmicro\n",
      "\n",
      " could not decide incident type automatically, please set playbook manually \n",
      "\n",
      "rule 1:  decide\n",
      "rule 2.4:  decide .  type\n",
      "second_api_param incident.\n",
      "decide . type\n",
      "\n",
      " close the current incident \n",
      "\n",
      "rule 1:  close\n",
      "rule 2.4:  close .  incident\n",
      "second_api_param current.\n",
      "close . incident\n",
      "\n",
      " list all demisto users via demisto rest api integration \n",
      "\n",
      "rule 1:  list\n",
      "rule 2.1:  list .  integration\n",
      "second_api_param demisto.rest.api.integration\n",
      "list . integration\n",
      "\n",
      " check if there are any users that weren't created through a saml login i.e. users created locally in demisto. \n",
      "\n",
      "rule 1:  check\n",
      "check . \n",
      "\n",
      " in an all-sso setup there shouldn't be any. \n",
      "\n",
      "rule 1:  be\n",
      "rule 2.1:  be .  setup\n",
      "second_api_param sso.setup\n",
      "be . setup\n",
      "\n",
      " alert and escalate \n",
      "\n",
      " . \n",
      "\n",
      " set incident severity to critical \n",
      "\n",
      "rule 1:  set\n",
      "rule 2.4:  set .  severity\n",
      "second_api_param incident.\n",
      "set . severity\n",
      "\n",
      " send an alert to the csirt slack channel \n",
      "\n",
      "rule 1:  send\n",
      "rule 2.4:  send .  alert\n",
      "second_api_param \n",
      "rule 3.1:  send .  alert . channel\n",
      "third_api_param csirt.slack.\n",
      "send . alert . channel\n",
      "\n",
      " send an sms alert using twilio. \n",
      "\n",
      "rule 1:  send\n",
      "rule 2.4:  send .  alert\n",
      "second_api_param sms.\n",
      "rule 3.5:  send .  alert .  twilio\n",
      "third_api_param \n",
      "send . alert . twilio\n",
      "\n",
      " auto-close this incident \n",
      "\n",
      "rule 1:  close\n",
      "rule 2.4:  close .  incident\n",
      "second_api_param \n",
      "close . incident\n",
      "\n",
      " check if the type or info of files is either executable or pdf \n",
      "\n",
      "rule 1:  check\n",
      "rule 2.5:  check .  type\n",
      "second_api_param \n",
      "check . type\n",
      "\n",
      " detonate the file in available sandboxes. \n",
      "\n",
      "rule 1:  detonate\n",
      "rule 2.4:  detonate .  file\n",
      "second_api_param \n",
      "rule 3.1:  detonate .  file . sandbox\n",
      "third_api_param available.\n",
      "detonate . file . sandbox\n",
      "\n",
      " detonates a file using anyrun \n",
      "\n",
      "rule 1:  detonate\n",
      "rule 2.4:  detonate .  file\n",
      "second_api_param \n",
      "rule 3.5:  detonate .  file .  anyrun\n",
      "third_api_param \n",
      "detonate . file . anyrun\n",
      "\n",
      " submit a file or url for analysis. \n",
      "\n",
      "rule 1:  submit\n",
      "rule 2.4:  submit .  file\n",
      "second_api_param \n",
      "submit . file\n",
      "\n",
      " get the report for a task created for a submitted file or url. \n",
      "\n",
      "rule 1:  get\n",
      "rule 2.4:  get .  report\n",
      "second_api_param \n",
      "rule 3.1:  get .  report . task\n",
      "third_api_param \n",
      "get . report . task\n",
      "\n",
      " verify that there is a valid instance of crowdstrike enabled. \n",
      "\n",
      "rule 1:  verify\n",
      "verify . \n",
      "\n",
      " \n",
      " \n",
      "\n",
      " . \n",
      "\n",
      " download the pdf report to the war room. \n",
      "\n",
      "rule 1:  download\n",
      "rule 2.4:  download .  report\n",
      "second_api_param pdf.\n",
      "rule 3.1:  download .  report . room\n",
      "third_api_param war.\n",
      "download . report . room\n",
      "\n",
      " set the file object into context. \n",
      "\n",
      "rule 1:  set\n",
      "rule 2.4:  set .  object\n",
      "second_api_param file.\n",
      "rule 3.1:  set .  object . context\n",
      "third_api_param \n",
      "set . object . context\n",
      "\n",
      " submits a sample to crowdstrike \n",
      "\n",
      "rule 1:  submit\n",
      "rule 2.4:  submit .  sample\n",
      "second_api_param \n",
      "submit . sample\n",
      "\n",
      " asserts there's a file to detonate \n",
      "\n",
      "rule 1:  assert\n",
      "assert . \n",
      "\n",
      " asserts the file type is supported \n",
      "\n",
      "rule 1:  assert\n",
      "assert . \n",
      "\n",
      " initiated the generic polling playbook that will do the polling command \n",
      "\n",
      "rule 1:  initiate\n",
      "rule 2.4:  initiate .  playbook\n",
      "second_api_param generic.polling.\n",
      "initiate . playbook\n",
      "\n",
      " detonate the file in crowdstrike falcon sandbox. \n",
      "\n",
      "rule 1:  detonate\n",
      "rule 2.4:  detonate .  file\n",
      "second_api_param \n",
      "rule 3.1:  detonate .  file . sandbox\n",
      "third_api_param crowdstrike.falcon.\n",
      "detonate . file . sandbox\n",
      "\n",
      " verify that there is a valid instance of crowdstrike falcon sandbox enabled. \n",
      "\n",
      "rule 1:  verify\n",
      "verify . \n",
      "\n",
      " check if file type is supported. \n",
      "\n",
      "rule 1:  check\n",
      "check . \n",
      "\n",
      " verify that there is a valid instance of fireeye ax enabled. \n",
      "\n",
      "rule 1:  verify\n",
      "verify . \n",
      "\n",
      " submits a sample to fireeye ax \n",
      "\n",
      "rule 1:  submit\n",
      "rule 2.4:  submit .  sample\n",
      "second_api_param \n",
      "rule 3.1:  submit .  sample . ax\n",
      "third_api_param fireeye.\n",
      "submit . sample . ax\n",
      "\n",
      " retrieve result data upon a file. \n",
      "\n",
      "rule 1:  datum\n",
      "rule 2.1:  result . datum\n",
      "first_api_param   second_api_param  retrieve.result.\n",
      "rule 3.1:  result .  datum . file\n",
      "third_api_param \n",
      "result . datum . file\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for sentence in sentence_list:\n",
    "    if(count < 210):\n",
    "        count = count + 1\n",
    "        continue\n",
    "    elif count == 250:\n",
    "        break\n",
    "    count = count + 1\n",
    "    print('\\n', sentence, '\\n')\n",
    "    doc = nlp(sentence)\n",
    "    first_api = second_api = third_api = first_api_feature = second_api_feature = third_api_feature = \"\"\n",
    "    first_api, second_api, third_api = generate_api(doc)\n",
    "    # generate_api_with_compound(doc)\n",
    "    if third_api:\n",
    "        if not second_api:\n",
    "            print(first_api, '.', third_api)\n",
    "        else:\n",
    "            print(first_api, '.', second_api, '.', third_api)\n",
    "    else:\n",
    "        print(first_api, '.', second_api)\n",
    "   # generate_api_param(doc, first_api, second_api, third_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'adverbial clause modifier'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('advcl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_api_param(doc, first_api, second_api, third_api):\n",
    "    for token in doc:\n",
    "        token_dep = token.dep_\n",
    "        token_pos = token.pos_\n",
    "        list_dep = ['compound', 'pobj', root, 'nsubj', 'dobj']\n",
    "        if token_dep == compound and token_pos == noun:\n",
    "            if token.head.pos_ in [verb, noun] and token.head.dep_ is root:\n",
    "                param = token.head.lemma_\n",
    "                third_api = token.lemma_\n",
    "                print('rule 5: ', third_api, '(', param, ')')  \n",
    "        elif token_dep == 'amod' and token_pos == 'ADJ':\n",
    "            if token.head.dep_ == 'dobj' or 'pboj' and token.head.pos_ == 'noun':\n",
    "                print('rule 6: ',token.lemma_, '.', token.head.lemma_)\n",
    "        elif token_dep == 'appos' and token_pos == 'PROP':\n",
    "            if token.head.dep_ == 'nsubjpass' and token.head.pos_ == noun:\n",
    "                 print('rule 6.1: ', token.lemma_)\n",
    "        elif token_dep == direct_obj and token_pos == noun:\n",
    "            if token.head.dep_ == 'acl' or 'ccomp' and token.head.pos_== verb:\n",
    "                if token.head.lemma_ == first_api:\n",
    "                    first_api_param = token\n",
    "                    print('rule 11: ',first_api, ' (', first_api_param, ')')\n",
    "                if token.head.lemma_ == second_api:\n",
    "                    second_api_param = token\n",
    "                    print('rule 11: ', second_api, ' (', second_api_param, ')')\n",
    "                if token.head.lemma_ == third_api:\n",
    "                    third_api_param = token\n",
    "                    print('rule 11: ', third_api, ' (', third_api_param, ')')\n",
    "        elif token_dep == prep and token.pos_ == adp:\n",
    "            if token.head.dep_ == 'appos' and token.head.pos_ == 'PRON':\n",
    "                third_api = token.head.lemma_\n",
    "                third_api_child = token.child.lemma_\n",
    "                print('rule 3 ', third_api, third_api_child)\n",
    "        elif token_dep == 'acl' or 'advcl' and token_pos == verb or noun:\n",
    "            if token.head.dep_ == root and token.head.pos_==verb:\n",
    "                #if len(child) > 0:\n",
    "                   # parameter_api = token.lemma_\n",
    "                        #print('rule 10: ', parameter_api)\n",
    "                if token.head.dep_ == 'pobj' and token.head.pos_ == noun:\n",
    "                    third_api = token.head.lemma_\n",
    "                    third_api_param = token.children\n",
    "                        #print('Rule 10 ', third_api, third_api_param)\n",
    "        elif token_dep == compound and token_pos == noun:\n",
    "            if token.head.pos_ in verb_noun and token.head.dep_ in list_dep:\n",
    "                    # api_feature = token.lemma_\n",
    "                    #param = token.head.lemma_\n",
    "                third_api_head = token.lemma_\n",
    "                third_api = token.head.lemma_\n",
    "                print('rule 3.1: ', third_api)\n",
    "        elif token_dep == compound and token_pos == noun:\n",
    "            if token.head.pos_ in verb_noun and token.head.dep_ in list_dep:\n",
    "                api_feature = token.lemma_\n",
    "                param = token.head.lemma_\n",
    "                # third_api_head = token.lemma_\n",
    "                # third_api = token.head.lemma_\n",
    "               \n",
    "                #if token.head.dep_ == nominal_subj:\n",
    "                    # third_api = token.head.lemma_\n",
    "                    # print('rule 3.1: ', first_api, '. ', second_api, '.', third_api)\n",
    "                #elif token.head.dep_ == direct_obj:\n",
    "                    # second_api = token.head.lemma_\n",
    "                    # print('rule 3.1: ', first_api, '. ', second_api, '.', third_api)\n",
    "                \n",
    "                # print('rule 3.1: ', first_api, '. ', second_api, '.', third_api)\n",
    "         # elif token.head.dep_ == 'acl' or 'ccomp' and token.head.pos_==verb:\n",
    "                    #  first_api = token.head.lemma_ \n",
    "               #if (token1.dep_==direct_obj or token.dep_ == 'nsubjpass') and token1.pos_== noun:\n",
    "                            #second_api = token1.lemma_\n",
    "                           # print('rule 3.1: ', first_api, '. ', second_api, '.', third_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " enrich accounts using one or more integrations \n",
      "\n",
      "rule 1:  enrich\n",
      "rule 2.4:  enrich .  account\n",
      "second_api_param \n",
      "rule 3.5:  enrich .  account .  integration\n",
      "third_api_param \n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for sentence in sentence_list:\n",
    "    if(count == 1):\n",
    "        break\n",
    "    count = count + 1\n",
    "    print('\\n', sentence, '\\n')\n",
    "    doc = nlp(sentence)\n",
    "    first_api = second_api = third_api = first_api_feature = second_api_feature = third_api_feature = \"\"\n",
    "    first_api, second_api, third_api = generate_api(doc)\n",
    "    # generate_api_with_compound(doc)\n",
    "    generate_api_param(doc, first_api, second_api, third_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
